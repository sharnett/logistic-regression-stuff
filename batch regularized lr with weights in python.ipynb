{
 "metadata": {
  "name": "",
  "signature": "sha256:922a22cb3af191cc2bce3b1c1c42799b3839b98c3c47539d74b31b67ce7f4515"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use binomial instead of bernoulli\n",
      "\n",
      "$l_i(\\theta) = - y_i \\log(p_i) - (1-y_i) \\log(1-p_i) $, $y_i \\in {0,1}$\n",
      "\n",
      "becomes\n",
      "\n",
      "$l_i(\\theta) = - k_i \\log(p_i) - (n_i-k_i) \\log(1-p_i) $\n",
      "\n",
      "where $p_i = f(x_i^T \\theta + \\theta_0)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$ r = n \\odot f(X^T \\theta + \\theta_0) - k $\n",
      "\n",
      "$ \\nabla_0 l = \\frac{\\sum r}{\\sum n} $\n",
      "\n",
      "$ \\nabla l = \\frac{X^T r + \\lambda \\theta}{\\sum n} $"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import fmin_l_bfgs_b\n",
      "\n",
      "ALPHA = 1.0\n",
      "FACTR = 1e10\n",
      "\n",
      "def logistic(x):\n",
      "    return 1/(1+np.exp(-x))\n",
      " \n",
      "def log_loss(yp_orig, n, k, eps=1e-15):\n",
      "    yp = yp_orig.copy()\n",
      "    yp[yp < eps] = eps\n",
      "    yp[yp > 1 - eps] = 1 - eps\n",
      "    x = -k * np.log(yp) - (n - k) * np.log(1 - yp)\n",
      "    return x.sum()/n.sum()\n",
      " \n",
      "def lbfgs_func(x, X, n, k, alpha=ALPHA):\n",
      "    intercept, theta = x[0], x[1:]\n",
      "    yp = logistic(X.dot(theta) + intercept)\n",
      "    penalty = alpha/2*(theta*theta).sum()/n.sum()\n",
      "    obj = log_loss(yp, n, k) + penalty\n",
      " \n",
      "    r = n*logistic(X.dot(theta)+intercept) - k\n",
      "    grad_intercept = r.sum()/n.sum()\n",
      "    grad_coefs = (X.T.dot(r) + alpha*theta)/n.sum()\n",
      "    grad = np.hstack([grad_intercept, grad_coefs])\n",
      " \n",
      "    return obj, grad\n",
      "\n",
      "#x0 = np.zeros(X.shape[1]+1)\n",
      "#x_opt, _, info = fmin_l_bfgs_b(lbfgs_func, x0, factr=FACTR, args=(X, n, k))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fake some data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import scipy.sparse\n",
      "from scipy.stats import norm as norm_dist, uniform"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_features = 25\n",
      "n_datapoints = 1e6\n",
      "sigma = 2\n",
      "intercept = 1.14\n",
      "theta = norm_dist.rvs(0, sigma, size=(n_features, 1))\n",
      "X = np.ceil(scipy.sparse.rand(n_datapoints, n_features, .3))  # binary features\n",
      "p = logistic(X.dot(theta) + intercept)\n",
      "y = uniform.rvs(size=(n_datapoints, 1)) < p"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 212
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = np.ones_like(y)[:, 0].astype(float)\n",
      "k = y[:, 0].astype(float)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 213
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "factr = 1e7\n",
      "alpha = sigma**-2\n",
      "\n",
      "x0 = np.zeros(X.shape[1]+1)\n",
      "x_opt, _, info = fmin_l_bfgs_b(lbfgs_func, x0, factr=factr, args=(X, n, k, alpha))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 4.77 s, sys: 627 ms, total: 5.39 s\n",
        "Wall time: 5.48 s\n"
       ]
      }
     ],
     "prompt_number": 214
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%precision 2\n",
      "x_opt[0], x_opt[1:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 215,
       "text": [
        "(1.15, array([-0.73, -3.81, -1.01, -3.41,  2.47, -1.54, -1.89,  1.08,  0.06,\n",
        "        -1.19, -0.49,  1.05,  1.65, -2.41, -1.91,  3.27,  0.74, -3.69,\n",
        "         2.96, -2.42,  1.12, -3.48,  0.57, -1.38,  0.34]))"
       ]
      }
     ],
     "prompt_number": 215
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%precision 2\n",
      "intercept, theta[:, 0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 216,
       "text": [
        "(1.14, array([-0.74, -3.81, -1.01, -3.41,  2.46, -1.54, -1.87,  1.09,  0.05,\n",
        "        -1.18, -0.5 ,  1.05,  1.66, -2.42, -1.91,  3.27,  0.73, -3.68,\n",
        "         2.96, -2.42,  1.12, -3.47,  0.56, -1.37,  0.34]))"
       ]
      }
     ],
     "prompt_number": 216
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "df = pd.DataFrame(X.todense())\n",
      "cols = list(df.columns)\n",
      "df['y'] = y.astype(float)\n",
      "summarized = df.groupby(cols).aggregate([np.sum, len]).reset_index()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 9.82 s, sys: 1.32 s, total: 11.1 s\n",
        "Wall time: 11.9 s\n"
       ]
      }
     ],
     "prompt_number": 217
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = summarized.values[:, :-2]\n",
      "n = summarized.values[:, -1]\n",
      "k = summarized.values[:, -2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 218
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "factr = 1e7\n",
      "alpha = sigma**-2\n",
      "\n",
      "x0 = np.zeros(X.shape[1]+1)\n",
      "x_opt, _, info = fmin_l_bfgs_b(lbfgs_func, x0, factr=factr, args=(X, n, k, alpha))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 3.14 s, sys: 459 ms, total: 3.6 s\n",
        "Wall time: 2.89 s\n"
       ]
      }
     ],
     "prompt_number": 219
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%precision 2\n",
      "x_opt[0], x_opt[1:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 206,
       "text": [
        "(1.14, array([-1.21,  0.28, -0.73, -2.34,  3.74]))"
       ]
      }
     ],
     "prompt_number": 206
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Old stuff"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Searching around the internet, I couldn't find an expression for the gradient of regularized logistic regression with sample weights. Here it is:\n",
      "\n",
      "$$r = f(X^T \\theta + \\theta_0) - y$$\n",
      "\n",
      "$$\\nabla_0 l = \\frac{r^T w}{\\sum w} $$\n",
      "\n",
      "$$\\nabla l = \\frac{X^T (w \\odot r) + \\lambda \\theta}{ \\sum w} $$\n",
      "\n",
      "where $\\odot$ indicates element-wise multiplication. The loss function here is\n",
      "\n",
      "$l(\\theta) = -w^T \\left(y \\odot \\log(p) + (1-y) \\odot \\log(1-p)\\right) / \\sum w $\n",
      "\n",
      "with $p = f(X^T \\theta + \\theta_0)$\n",
      "\n",
      "i.e. the $w$-weighted average of the usual log-loss.\n",
      "\n",
      "Note that I'm not regularizing the intercept term, $\\theta_0$.\n",
      "\n",
      "<hr>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scikit-learn's LogisticRegression classifier (which uses LIBLINEAR) doesn't support weighted samples. SGDClassifier does support weighted samples, but it can be tricky to tune. For my application, solving the optimization problem without scikit-learn using L-BFGS worked best."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#No regularization, no weights\n",
      "\n",
      "In other words, all samples have weight $1.0$. $M$ training examples and $N$ features,\n",
      "one of which is the intercept.\n",
      "\n",
      "$\\theta$ - real vector of $N$ coefficients\n",
      "\n",
      "$y$ - response variable, binary vector of $M$ failures or successes\n",
      "\n",
      "$X$ - $M$x$N$ design matrix. Element $(i, j)$ of $X$ is the value of the $j$th feature for training example $i$. One of the $N$ columns is a dummy column of ones for the intercept.\n",
      "\n",
      "$f(x) = \\frac{1}{1+e^{-x}}$ - logistic function\n",
      "\n",
      "$l(\\theta)$ - loss function. This is what we're trying to minimize when we're considering different coefficients $\\theta$.\n",
      "\n",
      "$p = f(X^T \\theta)$ - real vector of $M$ predictions.\n",
      "\n",
      "And finally\n",
      "\n",
      "$l_i(\\theta) = - y_i \\log(p_i) + (1-y_i) \\log(1-p_i) $\n",
      "\n",
      "$l(\\theta) = \\sum_i l_i(\\theta)/M$ is the average log-loss\n",
      "\n",
      "$r = f(X^T \\theta) - y$ - residual, real vector of $M$ elements\n",
      "\n",
      "$\\nabla l = X^T r $ - gradient of loss function, real vector of $N$ elements"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Standard regularization\n",
      "\n",
      "No regularization on the intercept. $M$ training examples and $N$ features.\n",
      "\n",
      "$\\theta \\in \\mathbb{R}^N$ - coefficients\n",
      "\n",
      "$\\theta_0 \\in \\mathbb{R}$ - intercept\n",
      "\n",
      "$y \\in \\{0, 1\\}^M$ - response variable ($M x 1$ vector)\n",
      "\n",
      "$w \\in (0, \\infty)^N $ - per example weight ($N x 1$ vector)\n",
      "\n",
      "$X \\in \\{0, 1\\}^{MxN}$ - $M x N$ design matrix\n",
      "\n",
      "$f(x) = \\frac{1}{1+e^{-x}}$ - logistic function\n",
      "\n",
      "$l(\\theta)$ - loss function\n",
      "\n",
      "$l(\\theta) = -w^T \\left(y \\odot \\log(p) + (1-y) \\odot \\log(1-p)\\right) / \\sum w $\n",
      "\n",
      "where $p = f(X^T \\theta + \\theta_0)$\n",
      "\n",
      "aka average of the usual log-loss, weighted by $w$\n",
      "\n",
      "$$r = f(X^T \\theta + \\theta_0) - y$$\n",
      "\n",
      "$$\\nabla_0 l = \\frac{r^T w}{\\sum w} $$\n",
      "\n",
      "$$\\nabla l = \\frac{X^T (w \\odot r) + \\lambda \\theta}{ \\sum w} $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Code"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy.optimize import fmin_l_bfgs_b"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def logistic(x):\n",
      "    return 1/(1+np.exp(-x))\n",
      "\n",
      "REG = 1.0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Old method"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def old_method(X, y, w, reg=REG):\n",
      "    def log_loss(yp_orig, yt, w, eps=1e-15):\n",
      "        yp = yp_orig.copy()\n",
      "        yp[yp < eps] = eps\n",
      "        yp[yp > 1 - eps] = 1 - eps\n",
      "        x = -w * (yt * np.log(yp) + (1 - yt) * np.log(1 - yp))\n",
      "        return x.sum()/w.sum()\n",
      "\n",
      "    def lbfgs_func(x, X, y, w, reg):\n",
      "        m, n = X.shape\n",
      "        intercept, theta = x[0], x[1:].reshape(n, 1)\n",
      "        yp = logistic(X.dot(theta) + intercept)[:, 0]\n",
      "        penalty = reg/2*(theta*theta).sum()/w.sum()\n",
      "        obj = log_loss(yp, y, w) + penalty\n",
      "\n",
      "        y, w = y.reshape((m, 1)), w.reshape((m, 1))\n",
      "        r = logistic(X.dot(theta)+intercept) - y\n",
      "        grad_intercept = np.average(r, weights=w)\n",
      "        grad_coefs = (X.T.dot(w*r) + reg*theta)/w.sum()\n",
      "        grad = np.hstack([grad_intercept, grad_coefs[:, 0]])\n",
      "\n",
      "        return obj, grad\n",
      "\n",
      "    def lr(X, y, w, reg, factr=1e10):\n",
      "        x0 = np.zeros(X.shape[1]+1)  # a coefficient for each feature, plus one for the intercept\n",
      "        x_opt, _, info = fmin_l_bfgs_b(lbfgs_func, x0, factr=factr, args=(X, y, w, reg))\n",
      "        assert info['warnflag'] == 0, 'l-BFGS did not converge'\n",
      "        return x_opt\n",
      "    \n",
      "    x = lr(X, y, w, reg)\n",
      "    intercept, theta = x[0], x[1:]\n",
      "    y_pred = logistic(X.dot(theta) + intercept)\n",
      "    return log_loss(y_pred, y, w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## New method: use n and k instead of y and w"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def new_method(X, n, k, reg=REG):\n",
      "    def log_loss(yp_orig, n, k, eps=1e-15):\n",
      "        yp = yp_orig.copy()\n",
      "        yp[yp < eps] = eps\n",
      "        yp[yp > 1 - eps] = 1 - eps\n",
      "        x = -k * np.log(yp) - (n - k) * np.log(1 - yp)\n",
      "        return x.sum()/n.sum()\n",
      "\n",
      "    def lbfgs_func(x, X, n, k, reg):\n",
      "        intercept, theta = x[0], x[1:]\n",
      "        yp = logistic(X.dot(theta) + intercept)\n",
      "        penalty = reg/2*(theta*theta).sum()/n.sum()\n",
      "        obj = log_loss(yp, n, k) + penalty\n",
      "\n",
      "        r = n*logistic(X.dot(theta)+intercept) - k\n",
      "        grad_intercept = r.sum()/n.sum()\n",
      "        grad_coefs = (X.T.dot(r) + reg*theta)/n.sum()\n",
      "        grad = np.hstack([grad_intercept, grad_coefs])\n",
      "\n",
      "        return obj, grad\n",
      "\n",
      "    def lr(X, n, k, reg, factr=1e10):\n",
      "        x0 = np.zeros(X.shape[1]+1)  # a coefficient for each feature, plus one for the intercept\n",
      "        x_opt, _, info = fmin_l_bfgs_b(lbfgs_func, x0, factr=factr, args=(X, n, k, reg))\n",
      "        assert info['warnflag'] == 0, 'l-BFGS did not converge'\n",
      "        return x_opt\n",
      "    \n",
      "    x = lr(X, n, k, reg)\n",
      "    intercept, theta = x[0], x[1:]\n",
      "    y_pred = logistic(X.dot(theta) + intercept)\n",
      "    return log_loss(y_pred, n, k)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def log_loss(yp_orig, n, k, eps=1e-15):\n",
      "    yp = yp_orig.copy()\n",
      "    yp[yp < eps] = eps\n",
      "    yp[yp > 1 - eps] = 1 - eps\n",
      "    x = -k * np.log(yp) - (n - k) * np.log(1 - yp)\n",
      "    return x.sum()/n.sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.zeros(X.shape[1]+1)\n",
      "k, n = f.raw_data.leads.values, f.raw_data.clicks.values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "M, N = X.shape\n",
      "intercept, theta = x[0], x[1:].reshape(N, 1)\n",
      "yp = logistic(X.dot(theta) + intercept)[:, 0]\n",
      "penalty = reg/2*(theta*theta).sum()/n.sum()\n",
      "obj = log_loss(yp, n, k) + penalty\n",
      "print(obj)\n",
      "n, k = n.reshape((M, 1)), k.reshape((M, 1))\n",
      "r = n*logistic(X.dot(theta)+intercept) - k\n",
      "grad_intercept = r.sum()/n.sum()\n",
      "grad_coefs = (X.T.dot(r) + reg*theta)/n.sum()\n",
      "grad = np.hstack([grad_intercept, grad_coefs[:, 0]])\n",
      "grad"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.69314718056\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "array([  3.94585197e-01,   1.26462011e-04,   1.75182761e-05, ...,\n",
        "         1.00857694e-04,   8.20019783e-04,   3.01097803e-06])"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "intercept, theta = x[0], x[1:]\n",
      "yp = logistic(X.dot(theta) + intercept)\n",
      "penalty = reg/2*(theta*theta).sum()/n.sum()\n",
      "obj = log_loss(yp, n, k) + penalty\n",
      "print(obj)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.69314718056\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = n*logistic(X.dot(theta)+intercept) - k\n",
      "grad_intercept = r.sum()/n.sum()\n",
      "grad_coefs = (X.T.dot(r) + reg*theta)/n.sum()\n",
      "grad = np.hstack([grad_intercept, grad_coefs])\n",
      "grad"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "array([  3.94585197e-01,   1.26462011e-04,   1.75182761e-05, ...,\n",
        "         1.00857694e-04,   8.20019783e-04,   3.01097803e-06])"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Do things"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.sparse\n",
      "from sys import path\n",
      "path.insert(0, '/home/seanharnett/Dropbox/cvr_redux/cvr-logistic-regression/cvrlr')\n",
      "from assemble_features import Features, get_feature_categories"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_file = '/home/seanharnett/Documents/vw/db_data_20141003.csv'\n",
      "data = pd.read_csv(data_file, nrows=100000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = Features()\n",
      "f.raw_data = data\n",
      "f.feature_categories = {0: [],\n",
      " 2: ['adconfiguration'],\n",
      " 3: ['provider'],\n",
      " 4: ['segment'],\n",
      " 7: ['target'],\n",
      " 11: ['provider', 'segment'],\n",
      " 13: ['provider', 'servicegroup']}\n",
      "f.encoded_data, f.reverse_encodings = f.encode_feature_dataframe()\n",
      "f.matrix, f.breaks = f.sparsify()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reg = 15"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def assemble_response_variable(leads, clicks):\n",
      "    \"\"\"\n",
      "    Creates the left-hand-side 'y' vector that we're trying to predict from the\n",
      "    design matrix X, as well as the vector of weights for each example.\n",
      "    \"\"\"\n",
      "    successes = leads\n",
      "    failures = clicks - leads\n",
      "    sample_weight = np.hstack([failures, successes])\n",
      "    y = np.hstack([np.zeros_like(failures), np.ones_like(successes)])\n",
      "    return y, sample_weight"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "y, w = assemble_response_variable(f.raw_data.leads, f.raw_data.clicks)\n",
      "X = scipy.sparse.vstack([f.matrix, f.matrix])\n",
      "print(old_method(X, y, w, reg))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.286539087713\n",
        "CPU times: user 2.93 s, sys: 357 ms, total: 3.29 s\n",
        "Wall time: 3.29 s\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "X = f.matrix\n",
      "k, n = f.raw_data.leads.values, f.raw_data.clicks.values\n",
      "print(new_method(X, n, k, reg))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.286539087686\n",
        "CPU times: user 1.46 s, sys: 87.8 ms, total: 1.55 s\n",
        "Wall time: 1.55 s\n"
       ]
      }
     ],
     "prompt_number": 35
    }
   ],
   "metadata": {}
  }
 ]
}